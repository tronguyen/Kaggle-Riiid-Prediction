{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "_cell_guid": "fd8ff387-9c14-47ff-955d-d0bd1fd9b10d",
    "_uuid": "8554fa27-784e-4f25-b627-fdaf4f3dd959"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datatable as dt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import gc\n",
    "import h5py\n",
    "import joblib\n",
    "\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "_cell_guid": "fbf87edc-04b9-435e-80f1-d6554c756d73",
    "_uuid": "53e8bc27-466e-47ab-9591-c8ffcd0d871c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:6\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "_cell_guid": "c8848800-3d8b-4aa7-b0c5-e807ee7efd20",
    "_uuid": "cd990caf-b201-4f01-b7d5-77abb022832e"
   },
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "        \n",
    "    def update(self, **kargs):\n",
    "        self.__dict__.update(kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "_cell_guid": "29a04d21-8112-44d0-827d-989ea07596ad",
    "_uuid": "45665757-b2d9-483e-8c2a-9dffbc5c54e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question shapes: (13523, 5)\n",
      "Lecture shapes: (418, 4)\n"
     ]
    }
   ],
   "source": [
    "path = './kaggle/input/riiid-test-answer-prediction'\n",
    "train_file = f'{path}/train.csv'\n",
    "train_dtypes = {'row_id': 'int64',\n",
    "              'timestamp': 'int64',\n",
    "              'user_id': 'int32',\n",
    "              'content_id': 'int16',\n",
    "              'content_type_id': 'int8',\n",
    "              'task_container_id': 'int16',\n",
    "              'user_answer': 'int8',\n",
    "              'answered_correctly': 'int8',\n",
    "              'prior_question_elapsed_time': 'float32', \n",
    "              'prior_question_had_explanation': 'boolean',\n",
    "             }\n",
    "\n",
    "questions = pd.read_csv(f'{path}/questions.csv')\n",
    "lectures = pd.read_csv(f'{path}/lectures.csv')\n",
    "print('Question shapes:', questions.shape)\n",
    "print('Lecture shapes:', lectures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['row_id', 'timestamp', 'user_id', 'content_id', 'content_type_id', 'task_container_id', 'user_answer', 'answered_correctly', 'prior_question_elapsed_time', 'prior_question_had_explanation']\n",
    "# chunks = pd.read_csv(train_file, chunksize=1e3, dtype=train_dtypes, header=None, names=colnames, index_col=False)\n",
    "chunks = pd.read_csv(train_file, chunksize=1e6, dtype=train_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "_cell_guid": "3bbb0f25-3253-480b-a4e3-8ab9adf15b7b",
    "_uuid": "fd79f190-cdc1-4284-b283-36814a278f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tags 189, n_parts 7\n"
     ]
    }
   ],
   "source": [
    "question_tags = list(map(lambda x: map(lambda v: int(v) + 1, str(x).split()) if str(x).strip() != 'nan' else [0], questions.tags.values))\n",
    "question_tags = list(set(itertools.chain(*question_tags)))\n",
    "\n",
    "n_tags = len(question_tags)\n",
    "n_parts = len(set(questions.part.unique()))\n",
    "\n",
    "print(f'n_tags {n_tags}, n_parts {n_parts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "_cell_guid": "f2408117-b91d-4c63-aa61-687b7228e13a",
    "_uuid": "e4c605ce-e959-4234-9b30-9f5f6d95fa5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'load_state': True, 'use_buffer': True, 'is_offline': False, 'batch_norm': False, 'is_test': False, 'n_chunks': 10, 'n_epoch': 1, 'learning_rate': 0.003, 'batch_size': 256, 'num_workers': 4, 'cuda': True, 'num_questions': 13523, 'num_lectures': 418, 'num_total_q_tags': 189, 'num_total_q_part': 7, 'dropout': 0.1, 'fm_n_layers': 1, 'fm_h_size': 32, 'emb_size': 64, 'sparse_size': 8, 'dnn_n_layers': 4, 'dnn_h_size': 64, 'input_size': 512, 'buffer_size_limit': 1000.0, 'limit_f1': 10, 'limit_f2': 5, 'limit_f3': 5, 'limit_f4': 5, 'n_features': 8, 'output_dir': '/kaggle/working/data', 'extra_dir': '/kaggle/input/deepfm-input'}\n"
     ]
    }
   ],
   "source": [
    "params_dict = {\n",
    "    'load_state': True,\n",
    "    'use_buffer': True,\n",
    "    'is_offline': False,\n",
    "    'batch_norm': False,\n",
    "    'is_test': False,\n",
    "    'n_chunks': 10,\n",
    "    'n_epoch': 1,\n",
    "    'learning_rate': 3e-3,\n",
    "    'batch_size': 256,\n",
    "    'num_workers': 4,\n",
    "    'cuda': torch.cuda.is_available(),\n",
    "    'num_questions': questions.question_id.nunique(),\n",
    "    'num_lectures': lectures.lecture_id.nunique(),\n",
    "    'num_total_q_tags': n_tags,\n",
    "    'num_total_q_part': n_parts,\n",
    "    'dropout': 0.1,\n",
    "    'fm_n_layers': 1,\n",
    "    'fm_h_size': 32,\n",
    "    'emb_size': 64,\n",
    "    'sparse_size': 8,\n",
    "    'dnn_n_layers': 4,\n",
    "    'dnn_h_size': 64,\n",
    "    'input_size': 512,\n",
    "    'buffer_size_limit': 1e3,\n",
    "    'limit_f1': 10, #lec-ques\n",
    "    'limit_f2': 5, # part\n",
    "    'limit_f3': 5, # tag\n",
    "    'limit_f4': 5, # user\n",
    "    'n_features': 8,\n",
    "    'output_dir': '/kaggle/working/data',\n",
    "    'extra_dir':'/kaggle/input/deepfm-input'\n",
    "}\n",
    "params = Params(**params_dict)\n",
    "print(params.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(train_part, n_tail=10):\n",
    "    valid = train_part.groupby('user_id').tail(n_tail)\n",
    "    train = train_part[~train_part.index.isin(valid.index)]\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LectureData(Dataset):\n",
    "    \n",
    "    def __init__(self, params, train_df=None, question_df=None, lecture_df=None, is_train=True):\n",
    "        # read init-data\n",
    "        self.params = params\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.scaler_ans_correct_mean = StandardScaler()\n",
    "        self.scaler_ans_correct_count = StandardScaler()\n",
    "        self.scaler_prior_question_time_mean = StandardScaler()\n",
    "        self.scaler_prior_question_elapsed_time = StandardScaler()\n",
    "#         self.scaler_timestamp = MinMaxScaler()\n",
    "\n",
    "        self.user_columns = ['user_id', 'ans_correct_sum', 'ans_correct_mean', 'ans_correct_count', 'lecture_set', 'lecture_tags_set', \n",
    "                             'prior_question_time_mean']\n",
    "\n",
    "        self.train_columns = ['user_id', 'content_id', 'content_type_id', 'task_container_id',\n",
    "                              'answered_correctly', 'prior_question_elapsed_time', 'prior_question_had_explanation']\n",
    "        \n",
    "        self.merge_columns = ['user_id', 'question_id', 'question_index', 'answered_correctly', 'ans_correct_mean', 'ans_correct_count',\n",
    "                              'part', 'tags_set', 'prior_question_time_mean', 'task_ans_correct_mean',\n",
    "                              'question_ans_correct_mean', 'question_ans_correct_count',\n",
    "                              'lecture_question_correct_mean', 'user_question_part_scores_mean', 'user_question_tag_scores_mean',\n",
    "                              'prior_question_had_explanation', 'prior_question_elapsed_time']\n",
    "        \n",
    "        self.prior_batch, self.current_batch, self.buffer_df, self.merge_df = None, None, None, None\n",
    "        self.user2idx, self.question2idx, self.lecture2idx = None, None, None\n",
    "        \n",
    "        # Restore all data\n",
    "        if params.load_state:\n",
    "            self.load_state()\n",
    "        else:\n",
    "            self.set_traindata(train_df, question_df, lecture_df)\n",
    "        \n",
    "    def proc_traindata(self, train_df):\n",
    "        \n",
    "        train_df['prior_question_elapsed_time'].fillna(0., inplace=True)\n",
    "        train_df['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "        train_df.loc[:, 'prior_question_had_explanation'] = train_df['prior_question_had_explanation'].astype(int)        \n",
    "        train_df.loc[:, 'prior_question_elapsed_time'] = train_df['prior_question_elapsed_time'].map(lambda x: np.log(x + 1.))\n",
    "#         train_df.loc[:, 'user_index'] = train_df['user_id'].apply(lambda x: self.user2idx[x])\n",
    "\n",
    "        return train_df\n",
    "    \n",
    "    def proc_questiondata(self, question_df):\n",
    "        \n",
    "        question_df.tags.fillna('-1', inplace=True)\n",
    "        question_df.loc[:, 'question_index'] = question_df['question_id'].map(lambda x: self.question2idx[x])\n",
    "        question_df['tags_set'] = question_df['tags'].map(lambda x: list(map(lambda s: int(s) + 1, str(x).split())))\n",
    "        question_df['part'] = question_df['part'].map(lambda x: x - 1)\n",
    "\n",
    "        return question_df\n",
    "        \n",
    "    def proc_lecturedata(self, lecture_df):\n",
    "        \n",
    "        lecture_df['lecture_index'] = lecture_df['lecture_id'].map(lambda x: self.lecture2idx[x])\n",
    "        lecture_df['lecture_id'] = lecture_df['lecture_id'].astype(int)\n",
    "\n",
    "        return lecture_df\n",
    "    \n",
    "    def load_state(self):\n",
    "        \n",
    "        self.user_df = pd.read_parquet(os.path.join(self.params.extra_dir,'user_df.gzip'))\n",
    "        self.question_df = pd.read_parquet(os.path.join(self.params.extra_dir,'question_df.gzip'))\n",
    "        self.lecture_df = pd.read_parquet(os.path.join(self.params.extra_dir,'lecture_df.gzip'))\n",
    "        \n",
    "        self.lecture_question_scores = pd.read_parquet(os.path.join(self.params.extra_dir,'lecture_question_scores.gzip'))\n",
    "        self.user_question_tag_scores = pd.read_parquet(os.path.join(self.params.extra_dir,'user_question_tag_scores.gzip'))\n",
    "        self.user_question_part_scores = pd.read_parquet(os.path.join(self.params.extra_dir,'user_question_part_scores.gzip'))\n",
    "        \n",
    "        self.user_df = self.user_df.set_index('user_id')\n",
    "        self.lecture_question_scores = self.lecture_question_scores.set_index(['question_id', 'lecture_id'])\n",
    "        self.user_question_tag_scores = self.user_question_tag_scores.set_index(['user_id', 'tag_id'])\n",
    "        self.user_question_part_scores = self.user_question_part_scores.set_index(['user_id', 'part'])\n",
    "        self.question_df = self.question_df.set_index('question_id')\n",
    "        self.lecture_df = self.lecture_df.set_index('lecture_id')\n",
    "        \n",
    "        f = h5py.File(os.path.join(self.params.extra_dir, 'data2idx.h5'), 'r')\n",
    "        self.user2idx = f['user2idx'][:]\n",
    "        self.n_users = len(self.user2idx)\n",
    "        self.user2idx = dict(zip(self.user2idx, range(self.n_users)))\n",
    "        \n",
    "        self.question2idx = f['question2idx'][:]\n",
    "        self.n_questions = len(self.question2idx)\n",
    "        self.question2idx = dict(zip(self.question2idx, range(self.n_questions)))\n",
    "        \n",
    "        self.lecture2idx = f['lecture2idx'][:]\n",
    "        self.n_lectures = len(self.lecture2idx)\n",
    "        self.lecture2idx = dict(zip(self.lecture2idx, range(self.n_lectures)))\n",
    "        f.close()\n",
    "        \n",
    "        self.scaler_ans_correct_count = joblib.load(os.path.join(self.params.extra_dir, 'scaler_ans_correct_count.save')) \n",
    "        self.scaler_prior_question_elapsed_time = joblib.load(os.path.join(self.params.extra_dir, 'scaler_prior_question_elapsed_time.save')) \n",
    "    \n",
    "    def init_info(self, train_df, question_df, lecture_df):\n",
    "        \n",
    "        self.user_list = list(train_df['user_id'].unique())\n",
    "        self.question_list = list(question_df['question_id'].unique())\n",
    "        self.lecture_list = list(lecture_df['lecture_id'].unique())\n",
    "\n",
    "        self.n_users = len(self.user_list)\n",
    "        self.n_questions = len(self.question_list)\n",
    "        self.n_lectures = len(self.lecture_list)\n",
    "        \n",
    "        self.user2idx = dict(zip(self.user_list, range(self.n_users)))\n",
    "        self.question2idx = dict(zip(self.question_list, range(self.n_questions)))\n",
    "        self.lecture2idx = dict(zip(self.lecture_list, range(self.n_lectures)))\n",
    "        \n",
    "    def fit_scaler(self, user_df):\n",
    "        \n",
    "        self.scaler_ans_correct_count.partial_fit(np.expand_dims(user_df['ans_correct_count'].values, 1))\n",
    "        self.scaler_prior_question_time_mean.partial_fit(np.expand_dims(user_df['prior_question_time_mean'].values, 1))\n",
    "        \n",
    "        \n",
    "    def transform_scaler(self, user_df):\n",
    "                \n",
    "        user_df['prior_question_elapsed_time'] = \\\n",
    "            self.scaler_prior_question_elapsed_time.transform(np.expand_dims(user_df['prior_question_elapsed_time'].values, 1)).squeeze(1)\n",
    "        \n",
    "#         user_df['timestamp'] = \\\n",
    "#             self.scaler_timestamp.transform(np.expand_dims(user_df['timestamp'].values, 1)).squeeze(1)\n",
    "\n",
    "        user_df['ans_correct_count'] = \\\n",
    "            self.scaler_ans_correct_count.transform(np.expand_dims(user_df['ans_correct_count'].values, 1)).squeeze(1)\n",
    "\n",
    "        user_df['prior_question_time_mean'] = \\\n",
    "            self.scaler_prior_question_time_mean.transform(np.expand_dims(user_df['prior_question_time_mean'].values, 1)).squeeze(1)\n",
    "        \n",
    "        return user_df\n",
    "\n",
    "    def feature_traindata(self, train_df):\n",
    "        \n",
    "        self.scaler_prior_question_elapsed_time.partial_fit(np.expand_dims(train_df['prior_question_elapsed_time'].values, 1))\n",
    "#         self.scaler_timestamp.partial_fit(np.expand_dims(train_df['timestamp'].values, 1))\n",
    "\n",
    "        grouped_by_task_id = train_df[train_df.content_type_id==0].groupby('task_container_id').agg({\n",
    "            'answered_correctly': ['sum', 'count', 'mean']\n",
    "        }).reset_index()\n",
    "        grouped_by_task_id.columns = ['task_container_id', 'task_ans_correct_sum', 'task_ans_correct_count', 'task_ans_correct_mean']\n",
    "        \n",
    "        return grouped_by_task_id\n",
    "\n",
    "    def feature_questiondata(self, train_df):\n",
    "        \n",
    "        question_train_df = train_df[train_df['content_type_id']==0]\n",
    "        \n",
    "        grouped_by_question = question_train_df.groupby('content_id').agg({\n",
    "            'answered_correctly': ['sum', 'count', 'mean']\n",
    "        }).reset_index() \n",
    "        grouped_by_question.columns = ['question_id', 'question_ans_correct_sum', 'question_ans_correct_count', 'question_ans_correct_mean']\n",
    "    \n",
    "        return grouped_by_question\n",
    "    \n",
    "    def feature_lecturedata(self, train_df):\n",
    "        \n",
    "        lecture_train_df = train_df[train_df['content_type_id']==1]\n",
    "        \n",
    "        grouped_by_lecture = lecture_train_df.groupby('content_id').agg({\n",
    "            'user_id': 'count'\n",
    "        }).reset_index()\n",
    "\n",
    "        grouped_by_lecture.columns = ['lecture_id', 'lecture_population']\n",
    "        \n",
    "        return grouped_by_lecture\n",
    "        \n",
    "    def feature_userdata(self, train_df):\n",
    "        \n",
    "        \n",
    "        # F: user ans correct mean \n",
    "        q_train_x = train_df[train_df['content_type_id']==0]        \n",
    "        q_train = q_train_x.merge(self.question_df, left_on='content_id', right_on='question_id', how='left', right_index=True)\n",
    "        question_grouped_by_user = q_train.groupby(['user_id']).agg({\n",
    "            'answered_correctly':['sum', 'count', 'mean'],\n",
    "            'prior_question_elapsed_time': 'mean',\n",
    "        }).reset_index()\n",
    "        question_grouped_by_user.columns = ['user_id', 'ans_correct_sum', 'ans_correct_count', 'ans_correct_mean',\n",
    "                                           'prior_question_time_mean']        \n",
    "        question_grouped_by_user.fillna(0, inplace=True)\n",
    "#         assert question_grouped_by_user.isnull().any().sum() == 0\n",
    "        \n",
    "        # new feature\n",
    "        user_question_part_scores = q_train.groupby(['user_id', 'part']).agg({\n",
    "            'answered_correctly': ['sum', 'count', 'mean']\n",
    "        }).reset_index()\n",
    "        user_question_part_scores.columns = ['user_id', 'part', 'user_question_part_scores_sum', \n",
    "                                             'user_question_part_scores_count', 'user_question_part_scores_mean']\n",
    "        user_question_part_scores['part'] = user_question_part_scores['part'].astype(int)\n",
    "        \n",
    "#         assert user_question_part_scores.isnull().any().sum() == 0\n",
    "        \n",
    "        # new feature\n",
    "        q_train_subset = q_train[['user_id', 'tags_set', 'answered_correctly']]\n",
    "        q_train_subset = q_train_subset.explode('tags_set')\n",
    "        q_train_subset.columns = ['user_id', 'tag_id', 'answered_correctly']\n",
    "        \n",
    "        user_question_tag_scores = q_train_subset.groupby(['user_id', 'tag_id']).agg({\n",
    "            'answered_correctly': ['sum', 'count', 'mean']\n",
    "        }).reset_index()\n",
    "        user_question_tag_scores.columns = ['user_id', 'tag_id', 'user_question_tag_scores_sum', \n",
    "                                            'user_question_tag_scores_count', 'user_question_tag_scores_mean']\n",
    "        user_question_tag_scores['tag_id'] = user_question_tag_scores['tag_id'].astype(int)\n",
    "                        \n",
    "#         assert user_question_tag_scores.isnull().any().sum() == 0\n",
    "        \n",
    "        l_train_x = train_df[train_df['content_type_id']==1]        \n",
    "        l_train = l_train_x.merge(self.lecture_df, left_on='content_id', right_on='lecture_id', how='left', right_index=True)\n",
    "        lecture_grouped_by_user = l_train.groupby(['user_id']).agg({\n",
    "            'lecture_id': lambda x: list(set(x)),\n",
    "            'tag': lambda x: list(set(x)),\n",
    "#             'part': lambda x: list(set(x)),\n",
    "#             'type_of': lambda x: list(set(x))\n",
    "        }).reset_index()\n",
    "\n",
    "        lecture_grouped_by_user.columns = ['user_id', 'lecture_set', 'lecture_tags_set']\n",
    "#                                           'lecture_part_set', 'lecture_typeof_set']\n",
    "        lecture_grouped_by_user.fillna(0, inplace=True)\n",
    "#         assert lecture_grouped_by_user.isnull().any().sum() == 0\n",
    "\n",
    "        # Gen user-feature\n",
    "        question_grouped_by_user = question_grouped_by_user.set_index('user_id')\n",
    "        lecture_grouped_by_user = lecture_grouped_by_user.set_index('user_id')\n",
    "        \n",
    "        user_df = question_grouped_by_user.join(lecture_grouped_by_user, on='user_id', how='outer').reset_index()\n",
    "        \n",
    "        if self.is_train:\n",
    "            self.fit_scaler(user_df)\n",
    "#             user_df = self.transform_scaler(user_df)\n",
    "                \n",
    "        # Get lecture-question scores\n",
    "        l_train = l_train_x[['user_id', 'content_id']]\n",
    "        l_train = l_train.rename(columns={'content_id': 'lecture_id'})\n",
    "        q_train = q_train_x[['user_id', 'content_id', 'answered_correctly']]\n",
    "        q_train = q_train.rename(columns={'content_id': 'question_id'})\n",
    "        \n",
    "        #optim        \n",
    "        lecture_question_scores = l_train.merge(q_train, on='user_id')\n",
    "        lecture_question_scores = lecture_question_scores.groupby(['lecture_id', 'question_id']).agg({\n",
    "            'answered_correctly': ['sum', 'count', 'mean']\n",
    "        }).reset_index()\n",
    "        lecture_question_scores.columns = ['lecture_id', 'question_id', \n",
    "                                           'lecture_question_correct_sum', 'lecture_question_correct_count', 'lecture_question_correct_mean']\n",
    "        lecture_question_scores['lecture_id'] = lecture_question_scores['lecture_id'].astype(int)\n",
    "        lecture_question_scores['question_id'] = lecture_question_scores['question_id'].astype(int)\n",
    "        \n",
    "#         assert lecture_question_scores.isnull().any().sum() == 0\n",
    "\n",
    "        # filter all NAN\n",
    "        user_df = user_df[self.user_columns]\n",
    "        \n",
    "#         user_df['tag_vec_score_sum'] = user_df['tag_vec_score_sum'].apply(lambda x: x if isinstance(x, list) else list(np.zeros(self.params.num_total_q_tags)))\n",
    "#         user_df['tag_vec_score_count'] = user_df['tag_vec_score_count'].apply(lambda x: x if isinstance(x, list) else list(np.zeros(self.params.num_total_q_tags)))\n",
    "        \n",
    "#         user_df['part_vec_score_sum'] = user_df['part_vec_score_sum'].apply(lambda x: x if isinstance(x, list) else list(np.zeros(self.params.num_total_q_part)))\n",
    "#         user_df['part_vec_score_count'] = user_df['part_vec_score_count'].apply(lambda x: x if isinstance(x, list) else list(np.zeros(self.params.num_total_q_part)))\n",
    "\n",
    "        user_df['lecture_set'] = user_df['lecture_set'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "        user_df['lecture_tags_set'] = user_df['lecture_tags_set'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "#         user_df['lecture_part_set'] = user_df['lecture_part_set'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "#         user_df['lecture_typeof_set'] = user_df['lecture_typeof_set'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "                             \n",
    "#         assert user_df.isnull().any().sum() == 0\n",
    "        \n",
    "        return (user_df, \n",
    "                lecture_question_scores, \n",
    "                user_question_part_scores, \n",
    "                user_question_tag_scores)\n",
    "    \n",
    "    def finalize_data(self, data_df):\n",
    "        # Filter out all\n",
    "        self.lecture_question_scores.loc[self.lecture_question_scores['lecture_question_correct_count'] < self.params.limit_f1, 'lecture_question_correct_mean'] = 0.5\n",
    "        self.user_question_part_scores.loc[self.user_question_part_scores['user_question_part_scores_count'] < self.params.limit_f2, 'user_question_part_scores_mean'] = 0.5\n",
    "        self.user_question_tag_scores.loc[self.user_question_tag_scores['user_question_tag_scores_count'] < self.params.limit_f3, 'user_question_tag_scores_mean'] = 0.5\n",
    "        self.user_df.loc[self.user_df['ans_correct_count'] < self.params.limit_f4, 'ans_correct_mean'] = 0.5\n",
    "\n",
    "        # Only filter content-type this step\n",
    "        data_df = data_df[data_df['content_type_id']==0]\n",
    "        data_df = data_df[self.train_columns]\n",
    "        data_df = data_df.rename(columns={'content_id': 'question_id'})\n",
    "       \n",
    "        # Get lectures\n",
    "        ql_df = data_df[['user_id', 'question_id']].set_index('user_id').join(self.user_df, on='user_id', how='left')\n",
    "        ql_df = ql_df.explode('lecture_set')\n",
    "        ql_df = ql_df.rename(columns={'lecture_set': 'lecture_id'})\n",
    "        ql_df.reset_index(inplace=True)\n",
    "        ql_df = ql_df.set_index(['question_id', 'lecture_id']).join(self.lecture_question_scores, on=['question_id', 'lecture_id'], how='left')\n",
    "        ql_df = ql_df.groupby(['user_id', 'question_id'])['lecture_question_correct_mean'].mean()\n",
    "        \n",
    "        # user-tags-scores\n",
    "        user_tags_df = data_df[['user_id', 'question_id']].set_index('question_id').join(self.question_df, on='question_id', how='left').reset_index()\n",
    "        user_tags_df = user_tags_df[['user_id', 'question_id', 'tags_set']]\n",
    "        user_tags_df = user_tags_df.explode('tags_set')\n",
    "        user_tags_df = user_tags_df.rename(columns={'tags_set': 'tag_id'})\n",
    "        user_tags_df = user_tags_df.set_index(['user_id', 'tag_id']).join(self.user_question_tag_scores, on=['user_id', 'tag_id'], how='left')\\\n",
    "                                   .groupby(['user_id', 'question_id'])['user_question_tag_scores_mean'].mean()\n",
    "        \n",
    "        # Final df\n",
    "        merge_df = data_df.join(ql_df, on=['user_id', 'question_id'], how='left')\\\n",
    "                          .join(user_tags_df, on=['user_id', 'question_id'], how='left')\\\n",
    "                          .join(self.question_df, on='question_id', how='left')\\\n",
    "                          .join(self.user_df, on='user_id', how='left')\\\n",
    "                          .join(self.user_question_part_scores, on=['user_id', 'part'], how='left')\\\n",
    "                          .join(self.task_df, on='task_container_id', how='left')\n",
    "                          \n",
    "        del data_df, ql_df, user_tags_df\n",
    "        merge_df = merge_df[self.merge_columns]\n",
    "\n",
    "        # scaler transform\n",
    "        merge_df = self.transform_scaler(merge_df)        \n",
    "        merge_df['ans_correct_mean'].fillna(0.65, inplace=True)\n",
    "        merge_df['ans_correct_count'].fillna(0., inplace=True)\n",
    "        merge_df['prior_question_time_mean'].fillna(0., inplace=True)\n",
    "        merge_df['question_ans_correct_mean'].fillna(0.5, inplace=True)\n",
    "        merge_df['lecture_question_correct_mean'].fillna(0.5, inplace=True)\n",
    "        merge_df['user_question_part_scores_mean'].fillna(0.5, inplace=True)\n",
    "        merge_df['user_question_tag_scores_mean'].fillna(0.5, inplace=True)\n",
    "        merge_df['task_ans_correct_mean'].fillna(0.5, inplace=True)\n",
    "        \n",
    "        merge_df['prior_question_elapsed_time'].fillna(0., inplace=True)\n",
    "        merge_df['prior_question_had_explanation'].fillna(0, inplace=True)\n",
    "        \n",
    "#         merge_df['timestamp'].fillna(0., inplace=True)\n",
    "#         merge_df['part'] = merge_df['part'].fillna(0).astype(int)\n",
    "#         merge_df['tags_set'] = merge_df['tags_set'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "        \n",
    "#         merge_df['tag_vec_score_count'] = merge_df['tag_vec_score_count'].apply(lambda x: x if isinstance(x, list) else list(np.zeros(self.params.num_total_q_tags)))\n",
    "#         merge_df['tag_vec_score_sum'] = merge_df['tag_vec_score_sum'].apply(lambda x: x if isinstance(x, list) else list(np.zeros(self.params.num_total_q_tags)))\n",
    "        \n",
    "#         merge_df['part_vec_score_count'] = merge_df['part_vec_score_count'].apply(lambda x: x if isinstance(x, list) else list(np.zeros(self.params.num_total_q_part)))\n",
    "#         merge_df['part_vec_score_sum'] = merge_df['part_vec_score_sum'].apply(lambda x: x if isinstance(x, list) else list(np.zeros(self.params.num_total_q_part)))\n",
    "        \n",
    "#         assert merge_df.isnull().any().sum() == 0\n",
    "        \n",
    "        return merge_df  \n",
    "    \n",
    "    def set_traindata(self, train_df, question_df, lecture_df):\n",
    "        # Fetch init info\n",
    "        self.init_info(train_df, question_df, lecture_df)\n",
    "        \n",
    "        # Refine all dfs\n",
    "        train_df = self.proc_traindata(train_df)\n",
    "        question_df = self.proc_questiondata(question_df)\n",
    "        lecture_df = self.proc_lecturedata(lecture_df)\n",
    "                \n",
    "        # Update all data\n",
    "        self.task_df = self.feature_traindata(train_df)\n",
    "        \n",
    "        question_stats_df = self.feature_questiondata(train_df)\n",
    "        question_stats_df.set_index('question_id', inplace=True)\n",
    "        question_df.set_index('question_id', inplace=True)\n",
    "        \n",
    "        self.question_df = question_df.merge(question_stats_df, on='question_id', how='left').reset_index()\n",
    "        self.question_df.fillna(0.5, inplace=True)\n",
    "#         assert self.question_df.isnull().any().sum() == 0, 'Question with NAN'\n",
    "        \n",
    "        lecture_stats_df = self.feature_lecturedata(train_df)\n",
    "        lecture_stats_df.set_index('lecture_id', inplace=True)\n",
    "        lecture_df.set_index('lecture_id', inplace=True)\n",
    "        \n",
    "        self.lecture_df = lecture_df.merge(lecture_stats_df, on='lecture_id', how='left').reset_index()\n",
    "        self.lecture_df.fillna(0., inplace=True)\n",
    "#         assert self.lecture_df.isnull().any().sum() == 0, 'Lecture with NAN'\n",
    "        \n",
    "        # update all question/lecture before user-df\n",
    "        (self.user_df, self.lecture_question_scores,\n",
    "            self.user_question_part_scores, self.user_question_tag_scores) = self.feature_userdata(train_df)\n",
    "        \n",
    "        # Fetch users w question only        \n",
    "        self.set_index()\n",
    "        self.merge_df = self.finalize_data(train_df)\n",
    "        \n",
    "    def update_newdata(self, batch_df, skip_stat=True):\n",
    "        \n",
    "        if skip_stat:\n",
    "            n_newusers, n_newquestions, n_newlectures = 0, 0, 0\n",
    "        else:\n",
    "            # fetch all new instances        \n",
    "            new_users = list(set(batch_df.user_id.unique()) - set(self.user2idx.keys()))\n",
    "            new_questions = list(set(batch_df.loc[batch_df['content_type_id']==0, 'content_id'].unique()) - set(self.question2idx.keys()))\n",
    "            new_lectures = list(set(batch_df.loc[batch_df['content_type_id']==1, 'content_id'].unique()) - set(self.lecture2idx.keys()))\n",
    "\n",
    "            n_newusers = len(new_users)\n",
    "            n_newquestions = len(new_questions)\n",
    "            n_newlectures = len(new_lectures)\n",
    "\n",
    "            new_user2idx = dict(zip(new_users, range(self.n_users, self.n_users + n_newusers)))\n",
    "            new_question2idx = dict(zip(new_questions, range(self.n_questions, self.n_questions + n_newquestions)))\n",
    "            new_lecture2idx = dict(zip(new_lectures, range(self.n_lectures, self.n_lectures + n_newlectures)))\n",
    "\n",
    "            self.user2idx.update(new_user2idx)\n",
    "            self.question2idx.update(new_question2idx)\n",
    "            self.lecture2idx.update(new_lecture2idx)\n",
    "\n",
    "            self.n_users = self.n_users + n_newusers\n",
    "            self.n_questions = self.n_questions + n_newquestions\n",
    "            self.n_lectures = self.n_lectures + n_newlectures\n",
    "        \n",
    "            # Append new questions + lectures\n",
    "            if n_newquestions > 0:\n",
    "                extra_questions = pd.DataFrame(list(new_question2idx.items()), \n",
    "                                               columns=['question_id', 'question_index'])\n",
    "                self.question_df = pd.concat([self.question_df, extra_questions], axis=0, ignore_index=True)\n",
    "\n",
    "            if n_newlectures > 0:\n",
    "                extra_lectures = pd.DataFrame(list(new_lecture2idx.items()), \n",
    "                                               columns=['lecture_id', 'lecture_index'])\n",
    "                self.lecture_df = pd.concat([self.lecture_df, extra_lectures], axis=0, ignore_index=True)\n",
    "            \n",
    "        # Add user-index\n",
    "        batch_df = self.proc_traindata(batch_df)                \n",
    "        return batch_df, n_newusers, n_newquestions, n_newlectures    \n",
    "    \n",
    "    def test_batch(self, batch_df):\n",
    "        # update new data\n",
    "        self.is_train = False\n",
    "        batch_df, n_newusers, n_newquestions, n_newlectures = self.update_newdata(batch_df)\n",
    "        \n",
    "        # fetch prior labels\n",
    "        gt_prior_batch = eval(batch_df.iloc[0][\"prior_group_answers_correct\"])\n",
    "        \n",
    "        # HERE stop updating first for 1st submission\n",
    "        if self.current_batch is not None and len(gt_prior_batch) > 0:\n",
    "            # save prior-batch with labels\n",
    "            self.prior_batch = self.current_batch\n",
    "            \n",
    "            # Assign label to prev-batch\n",
    "            self.prior_batch['answered_correctly'] = gt_prior_batch\n",
    "            self.prior_batch = self.prior_batch[self.train_columns]\n",
    "            \n",
    "            # add to buffer-df\n",
    "            if self.params.use_buffer:\n",
    "                self.buffer_df = pd.concat([self.buffer_df, self.prior_batch], axis=0, ignore_index=True)\n",
    "                del self.prior_batch\n",
    "             \n",
    "        else:\n",
    "            self.prior_batch = batch_df\n",
    "        \n",
    "        self.current_batch = batch_df\n",
    "        # create dummy labels\n",
    "        self.current_batch['answered_correctly'] = 0\n",
    "                            \n",
    "        # Update new-batch-data\n",
    "        self.merge_df = self.finalize_data(self.current_batch)       \n",
    "        \n",
    "        return n_newusers, n_newquestions, n_newlectures\n",
    "    \n",
    "    def set_batch(self, batch_df):\n",
    "        \n",
    "        self.is_train = True\n",
    "        batch_df, n_newusers, n_newquestions, n_newlectures = self.update_newdata(batch_df)\n",
    "        \n",
    "        # Update new-batch-data\n",
    "        self.merge_df = self.finalize_data(batch_df)    \n",
    "        return batch_df, n_newusers, n_newquestions, n_newlectures\n",
    "        \n",
    "    def finetune_batch(self):\n",
    "        \"\"\" Only finetune on previous batch-data with labels\n",
    "        \"\"\"\n",
    "        if self.buffer_df is not None and len(self.buffer_df) > params.buffer_size_limit:\n",
    "            print('--> Dataset activated finetune buffer')\n",
    "            self.is_train = True\n",
    "            \n",
    "            stat_buff, train_buff = split_data(self.buffer_df, n_tail=5)\n",
    "            self.agg_newdata(stat_buff)\n",
    "            self.merge_df = self.finalize_data(train_buff)\n",
    "            self.buffer_df = None\n",
    "        \n",
    "            # clean buffer\n",
    "            self.lecture_question_scores = self.lecture_question_scores.iloc[-100000:]\n",
    "            self.user_question_tag_scores = self.user_question_tag_scores.iloc[-100000:]\n",
    "            self.user_question_part_scores = self.user_question_part_scores.iloc[-100000:]\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def stats_data(self, train_df):\n",
    "        \n",
    "        # Fetch all local-stats\n",
    "        task_stats_df = self.feature_traindata(train_df)\n",
    "        question_stats_df = self.feature_questiondata(train_df)\n",
    "        lecture_stats_df = self.feature_lecturedata(train_df)\n",
    "        user_compound = self.feature_userdata(train_df)\n",
    "        \n",
    "        return train_df, question_stats_df, lecture_stats_df, task_stats_df, user_compound\n",
    "    \n",
    "    def reset_index(self):\n",
    "        \n",
    "        self.user_df = self.user_df.reset_index()\n",
    "        self.lecture_question_scores = self.lecture_question_scores.reset_index()\n",
    "        self.user_question_tag_scores = self.user_question_tag_scores.reset_index()\n",
    "        self.user_question_part_scores = self.user_question_part_scores.reset_index()\n",
    "        self.question_df = self.question_df.reset_index()\n",
    "        self.lecture_df = self.lecture_df.reset_index()\n",
    "        self.task_df = self.task_df.reset_index()\n",
    "        \n",
    "    def set_index(self):\n",
    "        \n",
    "        self.user_df = self.user_df.set_index('user_id')\n",
    "        self.lecture_question_scores = self.lecture_question_scores.set_index(['question_id', 'lecture_id'])\n",
    "        self.user_question_tag_scores = self.user_question_tag_scores.set_index(['user_id', 'tag_id'])\n",
    "        self.user_question_part_scores = self.user_question_part_scores.set_index(['user_id', 'part'])\n",
    "        self.question_df = self.question_df.set_index('question_id')\n",
    "        self.lecture_df = self.lecture_df.set_index('lecture_id')\n",
    "        self.task_df = self.task_df.set_index('task_container_id')\n",
    "\n",
    "    def agg_newdata(self, train_df, is_finetune=False):\n",
    "        self.reset_index()\n",
    "        # update info new-data\n",
    "        self.is_train = True\n",
    "        if is_finetune:\n",
    "            n_newusers, n_newquestions, n_newlectures = 0, 0, 0\n",
    "        else:\n",
    "            train_df, n_newusers, n_newquestions, n_newlectures = self.update_newdata(train_df)\n",
    "        \n",
    "        # run stats on new-data\n",
    "        train_df, question_stats_df, lecture_stats_df, task_stats_df, user_compound = self.stats_data(train_df)\n",
    "        new_user_df, new_lecture_question_scores, new_user_question_part_scores, new_user_question_tag_scores = user_compound\n",
    "        \n",
    "        self.task_df = pd.concat([self.task_df, task_stats_df], axis=0, ignore_index=True)\n",
    "        self.task_df = self.task_df.groupby('task_container_id').agg({\n",
    "            'task_ans_correct_sum': 'sum',\n",
    "            'task_ans_correct_count': 'sum'\n",
    "        }).reset_index()\n",
    "        self.task_df['task_ans_correct_mean'] = self.task_df['task_ans_correct_sum'] / self.task_df['task_ans_correct_count']\n",
    "        \n",
    "        self.user_df = pd.concat([self.user_df, new_user_df], axis=0, ignore_index=True)\n",
    "        self.user_df = self.user_df.groupby('user_id').agg({\n",
    "            'ans_correct_sum': 'sum',\n",
    "            'ans_correct_count': 'sum',\n",
    "            'prior_question_time_mean': 'mean',\n",
    "            'lecture_set': lambda x: list(set(itertools.chain(*x))),\n",
    "            'lecture_tags_set': lambda x: list(set(itertools.chain(*x))),\n",
    "#             'lecture_part_set': lambda x: list(set(itertools.chain(*x))),\n",
    "#             'lecture_typeof_set': lambda x: list(set(itertools.chain(*x))),\n",
    "#             'tag_vec_score_sum': lambda x: list(np.sum(np.array([*x]), 0)),\n",
    "#             'tag_vec_score_count': lambda x: list(np.sum(np.array([*x]), 0)),\n",
    "#             'part_vec_score_sum': lambda x: list(np.sum(np.array([*x]), 0)),\n",
    "#             'part_vec_score_count': lambda x: list(np.sum(np.array([*x]), 0)),\n",
    "        }).reset_index()\n",
    "#         pdb.set_trace()\n",
    "        self.user_df['ans_correct_mean'] = self.user_df['ans_correct_sum'] / self.user_df['ans_correct_count']\n",
    "                \n",
    "        self.lecture_question_scores = pd.concat([self.lecture_question_scores, \n",
    "                                                  new_lecture_question_scores], axis=0, ignore_index=True)\n",
    "        self.lecture_question_scores = self.lecture_question_scores.groupby(['lecture_id', 'question_id']).agg({\n",
    "            'lecture_question_correct_sum': 'sum',\n",
    "            'lecture_question_correct_count': 'sum',\n",
    "        }).reset_index()\n",
    "        \n",
    "        self.lecture_question_scores['lecture_question_correct_mean'] = self.lecture_question_scores['lecture_question_correct_sum'] / self.lecture_question_scores['lecture_question_correct_count']\n",
    "\n",
    "#         assert self.lecture_question_scores.isnull().any().sum() == 0\n",
    "        \n",
    "        self.user_question_part_scores = pd.concat([self.user_question_part_scores, \n",
    "                                                  new_user_question_part_scores], axis=0, ignore_index=True)\n",
    "        self.user_question_part_scores = self.user_question_part_scores.groupby(['user_id', 'part']).agg({\n",
    "            'user_question_part_scores_sum': 'sum',\n",
    "            'user_question_part_scores_count': 'sum',\n",
    "        }).reset_index()\n",
    "        self.user_question_part_scores['user_question_part_scores_mean'] = self.user_question_part_scores['user_question_part_scores_sum'] / self.user_question_part_scores['user_question_part_scores_count']\n",
    "        \n",
    "#         assert self.user_question_part_scores.isnull().any().sum() == 0\n",
    "        \n",
    "        self.user_question_tag_scores = pd.concat([self.user_question_tag_scores, \n",
    "                                                  new_user_question_tag_scores], axis=0, ignore_index=True)\n",
    "        self.user_question_tag_scores = self.user_question_tag_scores.groupby(['user_id', 'tag_id']).agg({\n",
    "            'user_question_tag_scores_sum': 'sum',\n",
    "            'user_question_tag_scores_count': 'sum',\n",
    "        }).reset_index()\n",
    "        self.user_question_tag_scores['user_question_tag_scores_mean'] = self.user_question_tag_scores['user_question_tag_scores_sum'] / self.user_question_tag_scores['user_question_tag_scores_count']        \n",
    "#         assert self.user_question_tag_scores.isnull().any().sum() == 0\n",
    "        \n",
    "        # update question-part\n",
    "        subset = self.question_df.loc[self.question_df['question_id'].isin(question_stats_df['question_id'].values), :].copy()\n",
    "        subset = subset.merge(question_stats_df, on='question_id', how='left')\n",
    "        subset['question_ans_correct_sum'] = subset[['question_ans_correct_sum_x','question_ans_correct_sum_y']].sum(axis=1)\n",
    "        subset['question_ans_correct_count'] = subset[['question_ans_correct_count_x','question_ans_correct_count_y']].sum(axis=1)        \n",
    "        subset['question_ans_correct_mean'] = subset['question_ans_correct_sum'] / subset['question_ans_correct_count']\n",
    "\n",
    "        self.question_df.loc[self.question_df['question_id'].isin(question_stats_df['question_id'].values), \n",
    "                             'question_ans_correct_mean'] = subset['question_ans_correct_mean'].values\n",
    "        self.question_df.loc[self.question_df['question_id'].isin(question_stats_df['question_id'].values), \n",
    "                             'question_ans_correct_count'] = subset['question_ans_correct_count'].values\n",
    "        self.question_df.loc[self.question_df['question_id'].isin(question_stats_df['question_id'].values), \n",
    "                             'question_ans_correct_sum'] = subset['question_ans_correct_sum'].values\n",
    "        \n",
    "        del subset, question_stats_df, lecture_stats_df\n",
    "        del new_lecture_question_scores, new_user_question_part_scores, new_user_question_tag_scores\n",
    "        # update lecture-part\n",
    "        \n",
    "        # Update new data for training\n",
    "        self.set_index()\n",
    "        \n",
    "        return n_newusers, n_newquestions, n_newlectures\n",
    "    \n",
    "    def cleanup(self):\n",
    "        print('--> Dataset cleaning ...')\n",
    "        if self.buffer_df is not None:\n",
    "            del self.buffer_df\n",
    "            self.buffer_df = None\n",
    "        if self.merge_df is not None:\n",
    "            del self.merge_df\n",
    "            self.merge_df = None\n",
    "        gc.collect()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.merge_df.shape[0] if self.merge_df is not None else 0\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ins = self.merge_df.iloc[index]\n",
    "        target = ins['answered_correctly']\n",
    "        \n",
    "        # get user-info\n",
    "#         user_id = ins['user_index']\n",
    "        user_ans_correct_mean = ins['ans_correct_mean']\n",
    "        user_ans_correct_count = ins['ans_correct_count'] \n",
    "        user_prior_question_time_mean = ins['prior_question_time_mean']\n",
    "        user_prior_question_elapsed_time = ins['prior_question_elapsed_time']\n",
    "        user_prior_question_had_explanation = ins['prior_question_had_explanation']\n",
    "        \n",
    "        # user-item interaction\n",
    "        user_item_correct_score = ins['lecture_question_correct_mean']\n",
    "        user_tag_correct_score = ins['user_question_tag_scores_mean']\n",
    "        user_part_correct_score = ins['user_question_part_scores_mean']\n",
    "        user_task_ans_correct = ins['task_ans_correct_mean']\n",
    "                \n",
    "        # item/question-info\n",
    "#         item_id = ins['question_index']\n",
    "#         item_part_vec = np.zeros(self.params.num_total_q_part)\n",
    "#         item_part_vec[list([ins['part']])] = 1.\n",
    "#         item_part = item_part_vec\n",
    "        \n",
    "#         item_tags_vec = np.zeros(self.params.num_total_q_tags)\n",
    "#         item_tags_vec[list(ins['tags_set'])] = 1.\n",
    "#         item_tags = item_tags_vec\n",
    "        item_ans_correct_mean = ins['question_ans_correct_mean']\n",
    "        \n",
    "        return (torch.FloatTensor([user_task_ans_correct]),\n",
    "                torch.FloatTensor([user_item_correct_score]), \n",
    "                torch.FloatTensor([user_ans_correct_mean]), \n",
    "                torch.FloatTensor([user_ans_correct_count]),\n",
    "                torch.FloatTensor([user_prior_question_time_mean]),\n",
    "                torch.FloatTensor([user_tag_correct_score]),\n",
    "                torch.FloatTensor([user_part_correct_score]),\n",
    "                torch.FloatTensor([user_prior_question_elapsed_time]),\n",
    "                torch.FloatTensor([user_prior_question_had_explanation]),\n",
    "                torch.FloatTensor([item_ans_correct_mean]),\n",
    "                torch.FloatTensor([target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "_cell_guid": "7634bd95-45a1-4dd2-8c7b-01247a9f8d36",
    "_uuid": "f7bd1d3f-a115-46b5-9954-168645ea6335"
   },
   "outputs": [],
   "source": [
    "class FM_COMP(nn.Module):\n",
    "    def __init__(self, n_layers, h_size, emb_size, sparse_size, n_features, dropout=0.0, batch_norm=False):\n",
    "        super(FM_COMP, self).__init__()\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        if batch_norm:\n",
    "            self.bnorm = nn.BatchNorm1d(h_size)\n",
    "            self.bn = nn.BatchNorm1d(emb_size)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.score = nn.Linear(h_size, 1)\n",
    "        \n",
    "        _in_size = emb_size\n",
    "        for i in range(n_layers):\n",
    "            _out_size = h_size * (n_layers - i)\n",
    "            self.layers.append(nn.Linear(_in_size, _out_size))\n",
    "            _in_size = _out_size\n",
    "            \n",
    "        self.sparse_layer = nn.Linear(sparse_size, 1)\n",
    "\n",
    "#         self.combine_layer = nn.Linear(1 + n_features**2, 1)\n",
    "        \n",
    "    def forward(self, x, sp_f):\n",
    "        \n",
    "        # bi-pooling part\n",
    "        summed_feature_emb = torch.sum(x, dim=1) # [None, K]\n",
    "        summed_squared_faeture_emb = torch.square(summed_feature_emb) # [None, K]\n",
    "        \n",
    "        square_feature_emb = torch.square(x) # [None, F, K]\n",
    "        squared_summed_feature_emb = torch.sum(square_feature_emb, dim=1)\n",
    "        bi_pool = 0.5 * (summed_squared_faeture_emb - squared_summed_feature_emb) # [None, K]\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            bi_pool = self.bn(bi_pool)\n",
    "        bi_pool = self.dropout(bi_pool)\n",
    "        \n",
    "        for i, h_layer in enumerate(self.layers):\n",
    "            bi_pool = h_layer(bi_pool)       \n",
    "            if self.batch_norm:\n",
    "                bi_pool = self.bnorm(bi_pool)\n",
    "            bi_pool = F.relu(bi_pool)\n",
    "            bi_pool = self.dropout(bi_pool)\n",
    "        \n",
    "        bi_pool = self.score(bi_pool)\n",
    "        \n",
    "#         x_norm = F.normalize(x, p=2, dim=-1)\n",
    "#         x_fm = torch.matmul(x_norm, x_norm.permute(0, 2, 1)) # [None, F, F]\n",
    "#         x_fm = torch.triu(x_fm, diagonal=1)\n",
    "#         x_fm = x_fm.reshape(-1, x.shape[1]**2)\n",
    "#         combine = torch.cat([x_fm, sparse_out], dim=-1)\n",
    "#         out = self.combine_layer(combine)\n",
    "        \n",
    "        # sparse-feature part\n",
    "        sparse_out = self.sparse_layer(sp_f)\n",
    "        out = bi_pool + sparse_out\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "_cell_guid": "29c72cc1-1413-4bfe-8192-53aeea034815",
    "_uuid": "cadd3ad3-cf48-406b-9624-d94e7c824c7a"
   },
   "outputs": [],
   "source": [
    "class DNN_COMP(nn.Module):\n",
    "    def __init__(self, n_layers, h_size, input_size, dropout=0.0, batch_norm=False):\n",
    "        super(DNN_COMP, self).__init__()\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        if batch_norm:\n",
    "            self.bnorm = nn.BatchNorm1d(h_size)\n",
    "        self.score = nn.Linear(h_size, 1)\n",
    "        \n",
    "        _in_size = input_size\n",
    "        for i in range(n_layers):\n",
    "            _out_size = h_size * (n_layers - i)\n",
    "            self.layers.append(nn.Linear(_in_size, _out_size))\n",
    "            _in_size = _out_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, h_layer in enumerate(self.layers):\n",
    "            x = h_layer(x)            \n",
    "            if self.batch_norm:\n",
    "                x = self.bnorm(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        out = self.score(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "_cell_guid": "4b8f4c81-2937-405a-9ff1-baf7038a020c",
    "_uuid": "aabb5f8b-8357-47ed-971b-4576e3e65964"
   },
   "outputs": [],
   "source": [
    "class DEEPFM(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(DEEPFM, self).__init__()\n",
    "        self.params = params\n",
    "        self.fm_comp = FM_COMP(params.fm_n_layers, \n",
    "                               params.fm_h_size, \n",
    "                               params.emb_size, \n",
    "                               params.sparse_size, \n",
    "                               params.n_features,\n",
    "                               dropout=params.dropout,\n",
    "                               batch_norm=params.batch_norm)\n",
    "        \n",
    "        self.dnn_comp = DNN_COMP(params.dnn_n_layers, \n",
    "                                 params.dnn_h_size, \n",
    "                                 params.input_size, \n",
    "                                 dropout=params.dropout,\n",
    "                                 batch_norm=params.batch_norm)\n",
    "        \n",
    "#         self.user_emb = nn.Embedding(params.num_users, params.emb_size)\n",
    "#         self.question_emb = nn.Embedding(params.num_questions, params.emb_size)\n",
    "        \n",
    "        self.ans_mean = nn.Linear(1, params.emb_size, bias=False)\n",
    "        self.ans_count = nn.Linear(1, params.emb_size, bias=False)\n",
    "                \n",
    "        self.prior_time_mean = nn.Linear(1, params.emb_size, bias=False)\n",
    "        self.question_correct_emb = nn.Linear(1, params.emb_size, bias=False)\n",
    "        \n",
    "        self.user_question_correct_emb = nn.Linear(1, params.emb_size, bias=False)\n",
    "        self.user_tag_correct_emb = nn.Linear(1, params.emb_size, bias=False)\n",
    "        self.user_part_correct_emb = nn.Linear(1, params.emb_size, bias=False)\n",
    "        self.task_ans_correct_emb = nn.Linear(1, params.emb_size, bias=False)\n",
    "        \n",
    "        self.prior_time = nn.Linear(1, params.emb_size, bias=False)\n",
    "        self.prior_question_explained = nn.Linear(1, params.emb_size, bias=False)\n",
    "        \n",
    "    def update_size(self, n_users, n_questions, n_lectures):\n",
    "        # add new users\n",
    "        if n_users > 0:\n",
    "            extra_users = torch.rand(n_users, self.params.emb_size).to(device) \\\n",
    "                          if self.params.cuda \\\n",
    "                          else torch.rand(n_users, self.params.emb_size)\n",
    "\n",
    "            self.user_emb.weight = torch.nn.Parameter(torch.cat([self.user_emb.weight, extra_users], dim=0))\n",
    "            self.user_emb.num_embeddings = self.user_emb.weight.shape[0]   \n",
    "            \n",
    "        if n_questions > 0:\n",
    "            extra_questions = torch.rand(n_questions, self.params.emb_size).to(device) \\\n",
    "                              if self.params.cuda \\\n",
    "                              else torch.rand(n_questions, self.params.emb_size)\n",
    "            \n",
    "            self.question_emb.weight = torch.nn.Parameter(torch.cat([self.question_emb.weight, extra_questions], dim=0))\n",
    "            self.question_emb.num_embeddings = self.question_emb.weight.shape[0]\n",
    "    \n",
    "    \n",
    "    def forward(self, x_batch):\n",
    "        (b_user_task_ans_correct, b_user_item_correct_score, b_ans_correct_mean, b_ans_correct_count, b_prior_question_time_mean,\n",
    "        b_user_tag_correct_score, b_user_part_correct_score, b_prior_time, b_question_explained,\n",
    "        b_item_correct_mean) = x_batch\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        input_sparse = torch.cat((b_user_task_ans_correct, b_user_item_correct_score, b_ans_correct_mean, b_ans_correct_count, b_prior_question_time_mean,\n",
    "                                  b_question_explained, b_prior_time, b_item_correct_mean), dim=1)\n",
    "        \n",
    "        # user\n",
    "        b_user_task_ans_correct = self.task_ans_correct_emb(b_user_task_ans_correct)\n",
    "        b_prior_question_time_mean = self.prior_time_mean(b_prior_question_time_mean)\n",
    "        b_ans_correct_mean = self.ans_mean(b_ans_correct_mean)\n",
    "        b_ans_correct_count = self.ans_count(b_ans_correct_count)\n",
    "        b_user_item_correct_score = self.user_question_correct_emb(b_user_item_correct_score)\n",
    "        b_user_tag_correct_score = self.user_tag_correct_emb(b_user_tag_correct_score)\n",
    "        b_user_part_correct_score = self.user_part_correct_emb(b_user_part_correct_score)\n",
    "        b_prior_time = self.prior_time(b_prior_time)\n",
    "        b_question_explained = self.prior_question_explained(b_question_explained)\n",
    "\n",
    "        # item\n",
    "#         b_item_id = self.question_emb(b_item_id).squeeze(1)\n",
    "#         b_item_part = self.part_emb(b_item_part)\n",
    "#         b_item_tags = self.tag_emb(b_item_tags)\n",
    "        b_item_correct_mean = self.question_correct_emb(b_item_correct_mean)\n",
    "        \n",
    "        input_emb = torch.cat((b_user_task_ans_correct, b_user_item_correct_score, b_ans_correct_mean, b_ans_correct_count, b_prior_question_time_mean,\n",
    "                               b_question_explained, b_prior_time, b_item_correct_mean), dim=-1)\n",
    "        \n",
    "        input_emb = input_emb.reshape(-1, self.params.n_features, self.params.emb_size)\n",
    "        \n",
    "        # FM-part\n",
    "        fm_out = self.fm_comp(input_emb, input_sparse)\n",
    "        \n",
    "        # Deep-part\n",
    "        input_emb = input_emb.reshape(-1, self.params.emb_size * self.params.n_features)\n",
    "        dnn_out = self.dnn_comp(input_emb)\n",
    "        \n",
    "        # Combine\n",
    "        out = torch.sigmoid(fm_out + dnn_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBMTrainer(object):\n",
    "    \n",
    "    def __init__(self, dataset, params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.lgbm_params = {\n",
    "            'objective': 'binary',\n",
    "            'boosting' : 'gbdt',\n",
    "            'max_bin': 800,\n",
    "            'learning_rate': 0.0175,\n",
    "            'num_leaves': 80\n",
    "        }\n",
    "        self.model_lgbm = None\n",
    "        \n",
    "        self.selected_columns = ['answered_correctly', 'ans_correct_mean', 'ans_correct_count',\n",
    "                                 'prior_question_time_mean', 'task_ans_correct_mean',\n",
    "                                 'question_ans_correct_mean', 'lecture_question_correct_mean', \n",
    "                                 'user_question_part_scores_mean', 'user_question_tag_scores_mean',\n",
    "                                 'prior_question_had_explanation', 'prior_question_elapsed_time']\n",
    "    \n",
    "    def incre_update(self, data_batch, stat_batch, val_batch):\n",
    "        \n",
    "        # Stat-data\n",
    "        self.dataset.agg_newdata(stat_batch)\n",
    "#         pdb.set_trace()\n",
    "        # Traing-data\n",
    "        data_batch, n_newusers, n_newquestions, n_newlectures = self.dataset.set_batch(data_batch)\n",
    "     \n",
    "        self.params.update(**{\n",
    "            'num_users': self.dataset.n_users,\n",
    "            'num_questions': self.dataset.n_questions,\n",
    "            'num_lectures': self.dataset.n_lectures\n",
    "        })\n",
    "        \n",
    "        print(f'[Train] New cases: {n_newusers}, {n_newquestions}, {n_newlectures}')\n",
    "        print(f'[Train] All cases: {self.params.num_users}, {self.params.num_questions}, {self.params.num_lectures}')\n",
    "                \n",
    "        df_train = self.dataset.merge_df[self.selected_columns]\n",
    "        df_train_x = df_train.loc[:, df_train.columns != 'answered_correctly']\n",
    "        df_train_y = df_train['answered_correctly']\n",
    "        lgb_train = lgb.Dataset(df_train_x, df_train_y, \n",
    "                                categorical_feature = ['part', 'prior_question_had_explanation'])\n",
    "        \n",
    "        # Valid data\n",
    "        _, n_newusers, n_newquestions, n_newlectures = self.dataset.set_batch(val_batch)\n",
    "        print(f'------ [Valid] new-users: {n_newusers}')\n",
    "        df_val = self.dataset.merge_df[self.selected_columns]\n",
    "        df_val_x = df_val.loc[:, df_val.columns != 'answered_correctly']\n",
    "        df_val_y = df_val['answered_correctly']\n",
    "        lgb_val = lgb.Dataset(df_val_x, df_val_y, \n",
    "                              categorical_feature = ['part', 'prior_question_had_explanation'], \n",
    "                              reference=lgb_train)\n",
    "    \n",
    "        self.model_lgbm = lgb.train(self.lgbm_params, lgb_train,\n",
    "                                    valid_sets=[lgb_train, lgb_val],\n",
    "                                    verbose_eval=50,\n",
    "                                    num_boost_round=10000,\n",
    "                                    early_stopping_rounds=12,\n",
    "                                    init_model = self.model_lgbm if self.model_lgbm is not None else None)\n",
    "        \n",
    "        y_pred = self.model_lgbm.predict(df_val_x)\n",
    "        y_true = np.array(df_val_y)\n",
    "        auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "        acc = metrics.accuracy_score(y_true, (y_pred >= 0.5).astype(int))\n",
    "        \n",
    "        print(f'AUC: {auc} -- ACC: {acc}')\n",
    "        \n",
    "        self.dataset.agg_newdata(data_batch)\n",
    "        \n",
    "        lgb.plot_importance(self.model_lgbm)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "_cell_guid": "4842f1e1-0d17-4ffa-ba35-79559ced3bfe",
    "_uuid": "29510dd4-5c35-4a00-8acd-ef1ad2ac0a68"
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \n",
    "    def __init__(self, dataset, params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.dataset = dataset\n",
    "        self.model = DEEPFM(params)\n",
    "               \n",
    "        if params.cuda:\n",
    "            print('Moving model to gpus ...')\n",
    "            self.model.to(device)\n",
    "            \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=params.learning_rate)\n",
    "        self.criteria = nn.BCELoss()\n",
    "                \n",
    "        \n",
    "    def get_dataloader(self, batch_size):\n",
    "        data_loader = DataLoader(self.dataset, \n",
    "                                 batch_size=batch_size, \n",
    "                                 drop_last=False,\n",
    "                                 num_workers=4,\n",
    "                                 shuffle=True)\n",
    "        return data_loader\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        # save model as .pt or .pth file\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "            \n",
    "    def load_model(self, model_path):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            checkpoint = torch.load(model_path)\n",
    "        else:\n",
    "            # this helps avoid errors when loading single-GPU-trained weights onto CPU-model\n",
    "            checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "            \n",
    "        self.model.load_state_dict(checkpoint)\n",
    "\n",
    "    def infer(self, databatch):\n",
    "        \n",
    "        # All test-cases\n",
    "        num_test = len(databatch.loc[databatch['content_type_id'] == 0])\n",
    "        if num_test == 0:\n",
    "            return []\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        n_users, n_questions, n_lectures, = self.dataset.test_batch(databatch)\n",
    "#         print(f'[Infer] New cases: {n_users}, {n_questions}, {n_lectures}')\n",
    "        \n",
    "        # add new entries to model\n",
    "#         self.model.update_size(n_users, n_questions, n_lectures)\n",
    "        \n",
    "        batch_size = min(num_test, self.params.batch_size)\n",
    "        test_loader = DataLoader(self.dataset, \n",
    "                                 batch_size=batch_size, \n",
    "                                 drop_last=False,\n",
    "                                 num_workers=4,\n",
    "                                 shuffle=False)\n",
    "        test_outputs = []\n",
    "        for i, test_batch in enumerate(test_loader):\n",
    "            input_batch = ()\n",
    "            for feature in test_batch[:-1]:\n",
    "                input_batch += (feature.to(device) if self.params.cuda else feature, )\n",
    "\n",
    "            output_batch = self.model(input_batch).detach()\n",
    "#             assert torch.isnan(output_batch).any() == False, 'NAN in prediction!'\n",
    "            test_outputs.append(output_batch.cpu().numpy() if self.params.cuda else output_batch.numpy())\n",
    "        \n",
    "        test_outputs = np.concatenate(test_outputs, axis=0).squeeze(1)\n",
    "#         assert num_test == len(test_outputs)\n",
    "        \n",
    "        return test_outputs\n",
    "    \n",
    "    def incre_update(self, data_batch, stat_batch, val_batch):\n",
    "        \n",
    "        self.dataset.agg_newdata(stat_batch)\n",
    "        \n",
    "        _, n_newusers, n_newquestions, n_newlectures = self.dataset.set_batch(data_batch)\n",
    "     \n",
    "        self.params.update(**{\n",
    "            'num_users': self.dataset.n_users,\n",
    "            'num_questions': self.dataset.n_questions,\n",
    "            'num_lectures': self.dataset.n_lectures\n",
    "        })\n",
    "        \n",
    "        print(f'[Train] New cases: {n_newusers}, {n_newquestions}, {n_newlectures}')\n",
    "        print(f'[Train] All cases: {self.params.num_users}, {self.params.num_questions}, {self.params.num_lectures}')\n",
    "\n",
    "#         self.model.update_size(n_newusers, n_newquestions, n_newlectures)\n",
    "        self.train()\n",
    "        \n",
    "        # VALID\n",
    "        self.val(val_batch)\n",
    "        \n",
    "        # Add the rest\n",
    "        self.dataset.agg_newdata(data_batch)\n",
    "        self.dataset.agg_newdata(val_batch)\n",
    "            \n",
    "    def finetune_batch(self):\n",
    "        # update batch-data for finetuning\n",
    "        is_finetune = self.dataset.finetune_batch()\n",
    "        if is_finetune:\n",
    "            batch_size = min(self.params.batch_size, len(self.dataset))\n",
    "            \n",
    "            # start finetune model\n",
    "            self.model.train()\n",
    "            train_loader = self.get_dataloader(batch_size)\n",
    "            self.train_step(train_loader, print_step=100, msg='Finetune')\n",
    "\n",
    "    def train_step(self, train_loader, print_step=200, msg='Train'):\n",
    "        self.model.train()\n",
    "        train_auc = AverageMeter()\n",
    "        for i, databatch in enumerate(train_loader):\n",
    "            \n",
    "            # Move to device\n",
    "            input_batch = ()\n",
    "            for feature in databatch[:-1]:\n",
    "                input_batch += (feature.to(device) if self.params.cuda else feature, )\n",
    "            b_target = databatch[-1].to(device) if self.params.cuda else databatch[-1]\n",
    "            \n",
    "            # FW model\n",
    "            output_batch = self.model(input_batch)     \n",
    "            loss = self.criteria(output_batch, b_target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "                        \n",
    "            if not self.params.is_test:\n",
    "                if self.params.cuda:\n",
    "                    b_target = b_target.cpu().numpy()\n",
    "                    output_batch = output_batch.detach().cpu().numpy()\n",
    "                else:\n",
    "                    b_target = b_target.numpy()\n",
    "                    output_batch = output_batch.detach().numpy()\n",
    "                try:\n",
    "                    acc = metrics.accuracy_score(b_target, (output_batch >= 0.5).astype(int))\n",
    "                    auc = metrics.roc_auc_score(b_target, output_batch)\n",
    "                    train_auc.update(auc, b_target.shape[0])\n",
    "                except:\n",
    "                    auc, acc = None, None\n",
    "                    pass\n",
    "\n",
    "                if print_step > 0 and i % print_step == 0:\n",
    "                    print(f'+++ [{msg}] Loss: {loss.item()} AUC: {auc} ACC: {acc}')\n",
    "                    \n",
    "        if not self.params.is_test:\n",
    "            print(f'>>> [{msg}] TOTAL AUC: {train_auc.avg}')\n",
    "            \n",
    "    def val_step(self, val_loader, print_step=300, msg='Valid'):\n",
    "        self.model.eval()\n",
    "        val_auc = AverageMeter()\n",
    "        for i, databatch in enumerate(val_loader):\n",
    "            \n",
    "            # Move to device\n",
    "            input_batch = ()\n",
    "            for feature in databatch[:-1]:\n",
    "                input_batch += (feature.to(device) if self.params.cuda else feature, )\n",
    "            b_target = databatch[-1].to(device) if self.params.cuda else databatch[-1]\n",
    "            \n",
    "            # FW model\n",
    "            output_batch = self.model(input_batch)     \n",
    "                       \n",
    "            if self.params.cuda:\n",
    "                b_target = b_target.cpu().numpy()\n",
    "                output_batch = output_batch.detach().cpu().numpy()\n",
    "            else:\n",
    "                b_target = b_target.numpy()\n",
    "                output_batch = output_batch.detach().numpy()\n",
    "\n",
    "            try:\n",
    "                acc = metrics.accuracy_score(b_target, (output_batch >= 0.5).astype(int))\n",
    "                auc = metrics.roc_auc_score(b_target, output_batch)\n",
    "                val_auc.update(auc, b_target.shape[0])\n",
    "            except:\n",
    "                auc, acc = None, None\n",
    "                pass\n",
    "            \n",
    "            if print_step > 0 and i % print_step == 0:\n",
    "                print(f'- [{msg}] AUC: {auc} ACC: {acc}')\n",
    "        \n",
    "        print(f'>>> [{msg}] TOTAL AUC: {val_auc.avg}')\n",
    "    \n",
    "    def val(self, val_batch):\n",
    "        _, n_newusers, n_newquestions, n_newlectures = self.dataset.set_batch(val_batch)\n",
    "        print(f'------ [Valid] new-users: {n_newusers}')\n",
    "        val_loader = self.get_dataloader(self.params.batch_size)\n",
    "        self.model.update_size(n_newusers, n_newquestions, n_newlectures)\n",
    "        self.val_step(val_loader, print_step=400)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        train_loader = self.get_dataloader(self.params.batch_size)\n",
    "        for epoch in range(self.params.n_epoch):\n",
    "            print(f'Epoch: {epoch}')\n",
    "            self.train_step(train_loader, print_step=200)    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "09b9a9ab-fc3e-4754-b7a7-1306e7ad3681",
    "_uuid": "fecc0823-9724-4461-b981-b9f77072f0af"
   },
   "source": [
    "# Training all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_train_to_test(train_part, group=0):\n",
    "    train_part.loc[train_part['answered_correctly']==-1, 'answered_correctly'] = 0\n",
    "    train_part['answered_correctly'] = train_part['answered_correctly'].astype(str)\n",
    "    train_part = train_part.rename(columns={'answered_correctly': 'prior_group_answers_correct'})\n",
    "    train_part.iloc[0, 7] = '[' + ','.join(train_part['prior_group_answers_correct'].values) + ']'\n",
    "#     train_part.loc[:, 'content_type_id'] = [1 if np.random.rand() > 1./1e4 else 0 for _ in range(train_part.shape[0])] \n",
    "    train_part.loc[:, 'content_type_id'] = [1 if np.random.rand() > 0.5 else 0 for _ in range(train_part.shape[0])] \n",
    "#     train_part.loc[:, 'content_type_id'] = 1\n",
    "\n",
    "    train_part['group_num'] = [group]*train_part.shape[0]\n",
    "    train_part = train_part.set_index('group_num')\n",
    "    \n",
    "    return train_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mini_chunks(data_chunk, csize=int(1e5)):\n",
    "    data_chunk = data_chunk.sort_values(by ='timestamp')\n",
    "    for i in range(0, int(data_chunk.shape[0]), csize):\n",
    "        yield data_chunk.iloc[i:i+csize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_all(ml_trainer):\n",
    "    ml_trainer.dataset.reset_index()\n",
    "    ml_trainer.dataset.user_df.to_parquet('./save/user_df.gzip',compression='gzip')\n",
    "    ml_trainer.dataset.question_df.to_parquet('./save/question_df.gzip',compression='gzip')\n",
    "    ml_trainer.dataset.lecture_df.to_parquet('./save/lecture_df.gzip',compression='gzip')\n",
    "    ml_trainer.dataset.task_df.to_parquet('./save/task_df.gzip',compression='gzip')\n",
    "\n",
    "    ml_trainer.dataset.lecture_question_scores.to_parquet('./save/lecture_question_scores.gzip',compression='gzip')\n",
    "    ml_trainer.dataset.user_question_tag_scores.to_parquet('./save/user_question_tag_scores.gzip',compression='gzip')\n",
    "    ml_trainer.dataset.user_question_part_scores.to_parquet('./save/user_question_part_scores.gzip',compression='gzip')\n",
    "\n",
    "    f = h5py.File('./save/data2idx.h5', 'w')\n",
    "    f.create_dataset('user2idx', data=list(ml_trainer.dataset.user2idx.keys()))\n",
    "    f.create_dataset('question2idx', data=list(ml_trainer.dataset.question2idx.keys()))\n",
    "    f.create_dataset('lecture2idx', data=list(ml_trainer.dataset.lecture2idx.keys()))\n",
    "    f.close()\n",
    "\n",
    "    joblib.dump(ml_trainer.dataset.scaler_ans_correct_mean, './save/scaler_ans_correct_mean.save') \n",
    "    joblib.dump(ml_trainer.dataset.scaler_ans_correct_count, './save/scaler_ans_correct_count.save') \n",
    "    joblib.dump(ml_trainer.dataset.scaler_prior_question_elapsed_time, './save/scaler_prior_question_elapsed_time.save')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING FROM SCRATCH ...\n",
      "Train: (48084, 10) / Stats: (29704, 10) / Valid: (22212, 10)\n",
      "***Training chunk-0:\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Moving model to gpus ...\n",
      "Train: (21042, 10) / Stats: (69739, 10) / Valid: (9219, 10)\n",
      "***Training chunk-0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.6635755300521851 AUC: 0.5602557478687678 ACC: 0.61328125\n",
      ">>> [Train] TOTAL AUC: 0.6470353298586643\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6447109588171839 ACC: 0.63671875\n",
      ">>> [Valid] TOTAL AUC: 0.6725854220265585\n",
      "Train: (16213, 10) / Stats: (77090, 10) / Valid: (6697, 10)\n",
      "***Training chunk-0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5874923467636108 AUC: 0.6689094477151668 ACC: 0.6875\n",
      ">>> [Train] TOTAL AUC: 0.6756540828978197\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7202775149795018 ACC: 0.69140625\n",
      ">>> [Valid] TOTAL AUC: 0.6821178194041345\n",
      "Train: (13273, 10) / Stats: (81274, 10) / Valid: (5453, 10)\n",
      "***Training chunk-0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.571252167224884 AUC: 0.7196843853820597 ACC: 0.68359375\n",
      ">>> [Train] TOTAL AUC: 0.6898763974428531\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6931910378830431 ACC: 0.65234375\n",
      ">>> [Valid] TOTAL AUC: 0.6973109551899943\n",
      "Train: (11262, 10) / Stats: (83984, 10) / Valid: (4754, 10)\n",
      "***Training chunk-0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5709611773490906 AUC: 0.6989202657807309 ACC: 0.7109375\n",
      ">>> [Train] TOTAL AUC: 0.6973521437101553\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6405716060888474 ACC: 0.58203125\n",
      ">>> [Valid] TOTAL AUC: 0.6936685652336131\n",
      "Train: (8958, 10) / Stats: (87238, 10) / Valid: (3804, 10)\n",
      "***Training chunk-0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.575045108795166 AUC: 0.7104789590254706 ACC: 0.71484375\n",
      ">>> [Train] TOTAL AUC: 0.6965451947870406\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7144043436601724 ACC: 0.66796875\n",
      ">>> [Valid] TOTAL AUC: 0.698613509311984\n",
      "Train: (7654, 10) / Stats: (89060, 10) / Valid: (3286, 10)\n",
      "***Training chunk-0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5822306871414185 AUC: 0.6996904024767803 ACC: 0.703125\n",
      ">>> [Train] TOTAL AUC: 0.6926771066079345\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6801943060935867 ACC: 0.625\n",
      ">>> [Valid] TOTAL AUC: 0.6964665117807542\n",
      "Train: (5952, 10) / Stats: (91502, 10) / Valid: (2546, 10)\n",
      "***Training chunk-0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.6139647960662842 AUC: 0.6668005354752344 ACC: 0.6328125\n",
      ">>> [Train] TOTAL AUC: 0.6976343628047775\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7417840375586854 ACC: 0.64453125\n",
      ">>> [Valid] TOTAL AUC: 0.6924085406260327\n",
      "Train: (4740, 10) / Stats: (93212, 10) / Valid: (2048, 10)\n",
      "***Training chunk-0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5641509294509888 AUC: 0.7424476888918791 ACC: 0.69921875\n",
      ">>> [Train] TOTAL AUC: 0.6973615719516222\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6814743589743589 ACC: 0.63671875\n",
      ">>> [Valid] TOTAL AUC: 0.6987044139711851\n",
      "Train: (4392, 10) / Stats: (93793, 10) / Valid: (1815, 10)\n",
      "***Training chunk-0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5610247254371643 AUC: 0.7806522725799131 ACC: 0.72265625\n",
      ">>> [Train] TOTAL AUC: 0.7062491688669731\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6722507773335872 ACC: 0.63671875\n",
      ">>> [Valid] TOTAL AUC: 0.6890219430284128\n",
      "Batch-Time elapsed: 178.84075593948364\n",
      "Train: (49116, 10) / Stats: (28237, 10) / Valid: (22647, 10)\n",
      "***Training chunk-1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.6299916505813599 AUC: 0.7138671875 ACC: 0.65234375\n",
      ">>> [Train] TOTAL AUC: 0.7254599092557087\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7299975532175189 ACC: 0.6484375\n",
      ">>> [Valid] TOTAL AUC: 0.7078061755274556\n",
      "Train: (21461, 10) / Stats: (69290, 10) / Valid: (9249, 10)\n",
      "***Training chunk-1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5702736377716064 AUC: 0.7283933421547462 ACC: 0.71484375\n",
      ">>> [Train] TOTAL AUC: 0.7255564291284956\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7445597165991903 ACC: 0.69921875\n",
      ">>> [Valid] TOTAL AUC: 0.7175927668536835\n",
      "Train: (16284, 10) / Stats: (76855, 10) / Valid: (6861, 10)\n",
      "***Training chunk-1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5901410579681396 AUC: 0.667367535744323 ACC: 0.68359375\n",
      ">>> [Train] TOTAL AUC: 0.7153256819514751\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6651062753036437 ACC: 0.62890625\n",
      ">>> [Valid] TOTAL AUC: 0.7012510947786961\n",
      "Train: (13625, 10) / Stats: (80668, 10) / Valid: (5707, 10)\n",
      "***Training chunk-1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5705718994140625 AUC: 0.7084717607973422 ACC: 0.72265625\n",
      ">>> [Train] TOTAL AUC: 0.7187626477015201\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6759765624999999 ACC: 0.625\n",
      ">>> [Valid] TOTAL AUC: 0.7065172603595911\n",
      "Train: (11385, 10) / Stats: (83836, 10) / Valid: (4779, 10)\n",
      "***Training chunk-1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5693925023078918 AUC: 0.7068181818181818 ACC: 0.734375\n",
      ">>> [Train] TOTAL AUC: 0.7144739122353881\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6882416396979503 ACC: 0.6640625\n",
      ">>> [Valid] TOTAL AUC: 0.7141205754052194\n",
      "Train: (9083, 10) / Stats: (87031, 10) / Valid: (3886, 10)\n",
      "***Training chunk-1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.6025427579879761 AUC: 0.6904953145917001 ACC: 0.6796875\n",
      ">>> [Train] TOTAL AUC: 0.7118666617650029\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7389676910953507 ACC: 0.69921875\n",
      ">>> [Valid] TOTAL AUC: 0.7153527003469264\n",
      "Train: (7962, 10) / Stats: (88545, 10) / Valid: (3493, 10)\n",
      "***Training chunk-1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.589780330657959 AUC: 0.7353792704111176 ACC: 0.69140625\n",
      ">>> [Train] TOTAL AUC: 0.7178453870885726\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7238202545068929 ACC: 0.7265625\n",
      ">>> [Valid] TOTAL AUC: 0.7012264137927828\n",
      "Train: (6328, 10) / Stats: (90981, 10) / Valid: (2691, 10)\n",
      "***Training chunk-1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5681527853012085 AUC: 0.7302697302697302 ACC: 0.73046875\n",
      ">>> [Train] TOTAL AUC: 0.6982629314134725\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6867971419695558 ACC: 0.65234375\n",
      ">>> [Valid] TOTAL AUC: 0.6819890445719606\n",
      "Train: (4847, 10) / Stats: (93046, 10) / Valid: (2107, 10)\n",
      "***Training chunk-1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.6069643497467041 AUC: 0.705954533712219 ACC: 0.6875\n",
      ">>> [Train] TOTAL AUC: 0.7101049780736419\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7271484375 ACC: 0.6796875\n",
      ">>> [Valid] TOTAL AUC: 0.7141900913216791\n",
      "Train: (2967, 10) / Stats: (95827, 10) / Valid: (1206, 10)\n",
      "***Training chunk-1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5511529445648193 AUC: 0.7594698243961515 ACC: 0.73046875\n",
      ">>> [Train] TOTAL AUC: 0.7147942851205337\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6544122414575634 ACC: 0.6640625\n",
      ">>> [Valid] TOTAL AUC: 0.6679162138644532\n",
      "Batch-Time elapsed: 478.655104637146\n",
      "Train: (52120, 10) / Stats: (22977, 10) / Valid: (24903, 10)\n",
      "***Training chunk-2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.6153711080551147 AUC: 0.7064039408866996 ACC: 0.66796875\n",
      "+++ [Train] Loss: 0.5967087149620056 AUC: 0.7549182855380819 ACC: 0.66796875\n",
      ">>> [Train] TOTAL AUC: 0.734615228617\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7271667791204074 ACC: 0.68359375\n",
      ">>> [Valid] TOTAL AUC: 0.7055475776845566\n",
      "Train: (23658, 10) / Stats: (65808, 10) / Valid: (10534, 10)\n",
      "***Training chunk-2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5785655975341797 AUC: 0.7370816599732263 ACC: 0.703125\n",
      ">>> [Train] TOTAL AUC: 0.7281664178871309\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7237978643814187 ACC: 0.7421875\n",
      ">>> [Valid] TOTAL AUC: 0.7216180121723176\n",
      "Train: (17907, 10) / Stats: (74268, 10) / Valid: (7825, 10)\n",
      "***Training chunk-2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5876211524009705 AUC: 0.6897832817337461 ACC: 0.671875\n",
      ">>> [Train] TOTAL AUC: 0.7155820737239343\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7902592807051405 ACC: 0.71484375\n",
      ">>> [Valid] TOTAL AUC: 0.7139268223901445\n",
      "Train: (14546, 10) / Stats: (79272, 10) / Valid: (6182, 10)\n",
      "***Training chunk-2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5884630084037781 AUC: 0.7073863636363636 ACC: 0.69140625\n",
      ">>> [Train] TOTAL AUC: 0.7263065601405265\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6933333333333335 ACC: 0.6484375\n",
      ">>> [Valid] TOTAL AUC: 0.7139088870636081\n",
      "Train: (12280, 10) / Stats: (82609, 10) / Valid: (5111, 10)\n",
      "***Training chunk-2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5567460060119629 AUC: 0.7352597179222566 ACC: 0.71875\n",
      ">>> [Train] TOTAL AUC: 0.7312988776323212\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6580640924612555 ACC: 0.63671875\n",
      ">>> [Valid] TOTAL AUC: 0.7149598105269516\n",
      "Train: (9894, 10) / Stats: (85916, 10) / Valid: (4190, 10)\n",
      "***Training chunk-2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.6102725863456726 AUC: 0.7004790801660812 ACC: 0.6796875\n",
      ">>> [Train] TOTAL AUC: 0.7209454338004638\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6819875776397516 ACC: 0.65625\n",
      ">>> [Valid] TOTAL AUC: 0.7145119985215485\n",
      "Train: (8786, 10) / Stats: (87410, 10) / Valid: (3804, 10)\n",
      "***Training chunk-2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5705999732017517 AUC: 0.6948853615520282 ACC: 0.75\n",
      ">>> [Train] TOTAL AUC: 0.7167943740482569\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.800630715862504 ACC: 0.7265625\n",
      ">>> [Valid] TOTAL AUC: 0.7248289703837327\n",
      "Train: (7098, 10) / Stats: (89846, 10) / Valid: (3056, 10)\n",
      "***Training chunk-2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5934860706329346 AUC: 0.6921078921078921 ACC: 0.6796875\n",
      ">>> [Train] TOTAL AUC: 0.7114912436552013\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7368653421633553 ACC: 0.6953125\n",
      ">>> [Valid] TOTAL AUC: 0.7003133157558679\n",
      "Train: (5919, 10) / Stats: (91471, 10) / Valid: (2610, 10)\n",
      "***Training chunk-2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5942683815956116 AUC: 0.7026307026307027 ACC: 0.6953125\n",
      ">>> [Train] TOTAL AUC: 0.7226088742430784\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7528153153153154 ACC: 0.69140625\n",
      ">>> [Valid] TOTAL AUC: 0.6919884286227899\n",
      "Train: (4265, 10) / Stats: (93946, 10) / Valid: (1789, 10)\n",
      "***Training chunk-2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.626514196395874 AUC: 0.6704805491990847 ACC: 0.65625\n",
      ">>> [Train] TOTAL AUC: 0.7122315883945383\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7782725961236906 ACC: 0.70703125\n",
      ">>> [Valid] TOTAL AUC: 0.7213606769147052\n",
      "Batch-Time elapsed: 811.7013437747955\n",
      "Train: (48259, 10) / Stats: (29815, 10) / Valid: (21926, 10)\n",
      "***Training chunk-3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.6142556667327881 AUC: 0.7325879765395895 ACC: 0.6875\n",
      ">>> [Train] TOTAL AUC: 0.7341269276904562\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6793587786259542 ACC: 0.609375\n",
      ">>> [Valid] TOTAL AUC: 0.7060197817762983\n",
      "Train: (21043, 10) / Stats: (69925, 10) / Valid: (9032, 10)\n",
      "***Training chunk-3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.6051256060600281 AUC: 0.6896371363190585 ACC: 0.65625\n",
      ">>> [Train] TOTAL AUC: 0.7264950396138805\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7529921059332825 ACC: 0.69921875\n",
      ">>> [Valid] TOTAL AUC: 0.7189164787544974\n",
      "Train: (15586, 10) / Stats: (77919, 10) / Valid: (6495, 10)\n",
      "***Training chunk-3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5825940370559692 AUC: 0.6921164772727272 ACC: 0.69140625\n",
      ">>> [Train] TOTAL AUC: 0.7285273567019288\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7669754072181412 ACC: 0.71484375\n",
      ">>> [Valid] TOTAL AUC: 0.7222750588927094\n",
      "Train: (12736, 10) / Stats: (82024, 10) / Valid: (5240, 10)\n",
      "***Training chunk-3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5777285099029541 AUC: 0.6896719827286023 ACC: 0.703125\n",
      ">>> [Train] TOTAL AUC: 0.7291978021716219\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7206485904595303 ACC: 0.6875\n",
      ">>> [Valid] TOTAL AUC: 0.722549490359411\n",
      "Train: (10120, 10) / Stats: (85612, 10) / Valid: (4268, 10)\n",
      "***Training chunk-3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.529882550239563 AUC: 0.7265471958209389 ACC: 0.76171875\n",
      ">>> [Train] TOTAL AUC: 0.7227390269699373\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7142590866728797 ACC: 0.66796875\n",
      ">>> [Valid] TOTAL AUC: 0.7154212859977842\n",
      "Train: (8719, 10) / Stats: (87646, 10) / Valid: (3635, 10)\n",
      "***Training chunk-3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.572097659111023 AUC: 0.7665461006409037 ACC: 0.70703125\n",
      ">>> [Train] TOTAL AUC: 0.7223952199644272\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7786204642039882 ACC: 0.71484375\n",
      ">>> [Valid] TOTAL AUC: 0.7170370932085824\n",
      "Train: (6913, 10) / Stats: (90162, 10) / Valid: (2925, 10)\n",
      "***Training chunk-3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5334320068359375 AUC: 0.7741277290348908 ACC: 0.7421875\n",
      ">>> [Train] TOTAL AUC: 0.7244351532583805\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7361882968290292 ACC: 0.73828125\n",
      ">>> [Valid] TOTAL AUC: 0.7163472462166861\n",
      "Train: (5278, 10) / Stats: (92487, 10) / Valid: (2235, 10)\n",
      "***Training chunk-3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5736026763916016 AUC: 0.7528416429863084 ACC: 0.69921875\n",
      ">>> [Train] TOTAL AUC: 0.7122528449209399\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7107514880952381 ACC: 0.62109375\n",
      ">>> [Valid] TOTAL AUC: 0.6879573029811072\n",
      "Train: (4641, 10) / Stats: (93355, 10) / Valid: (2004, 10)\n",
      "***Training chunk-3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5841841101646423 AUC: 0.7287248144220573 ACC: 0.6953125\n",
      ">>> [Train] TOTAL AUC: 0.7346743952222693\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7090909090909092 ACC: 0.62109375\n",
      ">>> [Valid] TOTAL AUC: 0.7147285197517651\n",
      "Train: (3332, 10) / Stats: (95224, 10) / Valid: (1444, 10)\n",
      "***Training chunk-3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5710919499397278 AUC: 0.6995896032831738 ACC: 0.70703125\n",
      ">>> [Train] TOTAL AUC: 0.7310205976950043\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6704101562500001 ACC: 0.640625\n",
      ">>> [Valid] TOTAL AUC: 0.6968882297394956\n",
      "Batch-Time elapsed: 1161.9799461364746\n",
      "Train: (48366, 10) / Stats: (29469, 10) / Valid: (22165, 10)\n",
      "***Training chunk-4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5886649489402771 AUC: 0.7517113783533764 ACC: 0.67578125\n",
      ">>> [Train] TOTAL AUC: 0.7352859224847663\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6694214876033058 ACC: 0.62890625\n",
      ">>> [Valid] TOTAL AUC: 0.7098500645475654\n",
      "Train: (21444, 10) / Stats: (69399, 10) / Valid: (9157, 10)\n",
      "***Training chunk-4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.576583981513977 AUC: 0.7524522799575822 ACC: 0.7109375\n",
      ">>> [Train] TOTAL AUC: 0.7215339255186558\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7566325190438666 ACC: 0.71875\n",
      ">>> [Valid] TOTAL AUC: 0.7278735873996999\n",
      "Train: (16139, 10) / Stats: (77076, 10) / Valid: (6785, 10)\n",
      "***Training chunk-4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.561362624168396 AUC: 0.7787114845938377 ACC: 0.72265625\n",
      ">>> [Train] TOTAL AUC: 0.7222058374519074\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7199060854115108 ACC: 0.6875\n",
      ">>> [Valid] TOTAL AUC: 0.72115139143062\n",
      "Train: (12921, 10) / Stats: (81704, 10) / Valid: (5375, 10)\n",
      "***Training chunk-4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.6296743154525757 AUC: 0.6670572916666667 ACC: 0.67578125\n",
      ">>> [Train] TOTAL AUC: 0.7277751486046211\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7041111754487551 ACC: 0.66015625\n",
      ">>> [Valid] TOTAL AUC: 0.718185332508879\n",
      "Train: (10142, 10) / Stats: (85572, 10) / Valid: (4286, 10)\n",
      "***Training chunk-4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5696973204612732 AUC: 0.7269944909202205 ACC: 0.703125\n",
      ">>> [Train] TOTAL AUC: 0.7212376866514552\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6403903903903904 ACC: 0.6015625\n",
      ">>> [Valid] TOTAL AUC: 0.6994755087181295\n",
      "Train: (9011, 10) / Stats: (87185, 10) / Valid: (3804, 10)\n",
      "***Training chunk-4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5171425342559814 AUC: 0.7799647266313934 ACC: 0.76953125\n",
      ">>> [Train] TOTAL AUC: 0.7404964917190481\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6897552609011148 ACC: 0.65234375\n",
      ">>> [Valid] TOTAL AUC: 0.7217135066323981\n",
      "Train: (7571, 10) / Stats: (89240, 10) / Valid: (3189, 10)\n",
      "***Training chunk-4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5644018054008484 AUC: 0.729858803986711 ACC: 0.703125\n",
      ">>> [Train] TOTAL AUC: 0.7136402100410714\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6831831831831832 ACC: 0.66796875\n",
      ">>> [Valid] TOTAL AUC: 0.7160854883954191\n",
      "Train: (6199, 10) / Stats: (91237, 10) / Valid: (2564, 10)\n",
      "***Training chunk-4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.593055009841919 AUC: 0.6581861508270255 ACC: 0.66796875\n",
      ">>> [Train] TOTAL AUC: 0.717514913412507\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7839556277056277 ACC: 0.73828125\n",
      ">>> [Valid] TOTAL AUC: 0.7298976085242991\n",
      "Train: (5101, 10) / Stats: (92748, 10) / Valid: (2151, 10)\n",
      "***Training chunk-4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5699625015258789 AUC: 0.7133505118740859 ACC: 0.69140625\n",
      ">>> [Train] TOTAL AUC: 0.7229386898529627\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7246992215145082 ACC: 0.67578125\n",
      ">>> [Valid] TOTAL AUC: 0.717979547837394\n",
      "Train: (3954, 10) / Stats: (94402, 10) / Valid: (1644, 10)\n",
      "***Training chunk-4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5812650918960571 AUC: 0.7612903225806451 ACC: 0.703125\n",
      ">>> [Train] TOTAL AUC: 0.731879825559731\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.689120151371807 ACC: 0.66015625\n",
      ">>> [Valid] TOTAL AUC: 0.7098923244214699\n",
      "Batch-Time elapsed: 1539.8255388736725\n",
      "Train: (50108, 10) / Stats: (26289, 10) / Valid: (23603, 10)\n",
      "***Training chunk-5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.6237910985946655 AUC: 0.7157869012707723 ACC: 0.66796875\n",
      ">>> [Train] TOTAL AUC: 0.7375010318717737\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.6762906777587472 ACC: 0.61328125\n",
      ">>> [Valid] TOTAL AUC: 0.7039002541932299\n",
      "Train: (22513, 10) / Stats: (67658, 10) / Valid: (9829, 10)\n",
      "***Training chunk-5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5834735631942749 AUC: 0.7291378059238736 ACC: 0.69140625\n",
      ">>> [Train] TOTAL AUC: 0.7339752535482956\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7284308048639259 ACC: 0.65625\n",
      ">>> [Valid] TOTAL AUC: 0.7239740711717707\n",
      "Train: (16680, 10) / Stats: (76314, 10) / Valid: (7006, 10)\n",
      "***Training chunk-5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.576947808265686 AUC: 0.7145917001338686 ACC: 0.67578125\n",
      ">>> [Train] TOTAL AUC: 0.7218920272901239\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7262264150943396 ACC: 0.6640625\n",
      ">>> [Valid] TOTAL AUC: 0.7045590364389477\n",
      "Train: (13715, 10) / Stats: (80628, 10) / Valid: (5657, 10)\n",
      "***Training chunk-5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.6049433350563049 AUC: 0.7029837251356239 ACC: 0.6796875\n",
      ">>> [Train] TOTAL AUC: 0.7301039309256719\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7569391392920803 ACC: 0.6875\n",
      ">>> [Valid] TOTAL AUC: 0.714754050764528\n",
      "Train: (11281, 10) / Stats: (84000, 10) / Valid: (4719, 10)\n",
      "***Training chunk-5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5477458238601685 AUC: 0.7451436388508892 ACC: 0.71875\n",
      ">>> [Train] TOTAL AUC: 0.7246474110643106\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7033112582781457 ACC: 0.69921875\n",
      ">>> [Valid] TOTAL AUC: 0.7222211985915958\n",
      "Train: (9871, 10) / Stats: (85797, 10) / Valid: (4332, 10)\n",
      "***Training chunk-5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n",
      "+++ [Train] Loss: 0.5418368577957153 AUC: 0.7772273821809426 ACC: 0.7265625\n",
      ">>> [Train] TOTAL AUC: 0.7289188153265111\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7248535777947543 ACC: 0.6796875\n",
      ">>> [Valid] TOTAL AUC: 0.7189000955618998\n",
      "Train: (8732, 10) / Stats: (87617, 10) / Valid: (3651, 10)\n",
      "***Training chunk-5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] New cases: 0, 0, 0\n",
      "[Train] All cases: 3603, 13523, 418\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f7ae9ad4820>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/logging/__init__.py\", line 223, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ [Train] Loss: 0.5756256580352783 AUC: 0.7147951288434368 ACC: 0.68359375\n",
      ">>> [Train] TOTAL AUC: 0.7182633719540252\n",
      "------ [Valid] new-users: 0\n",
      "- [Valid] AUC: 0.7126157777068028 ACC: 0.69921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-483-8d262aaa8ee8>\", line 55, in <module>\n",
      "    trainer.incre_update(train_part, stat_part, valid_part)\n",
      "  File \"<ipython-input-479-075ce0d10d0c>\", line 94, in incre_update\n",
      "    self.val(val_batch)\n",
      "  File \"<ipython-input-479-075ce0d10d0c>\", line 193, in val\n",
      "    self.val_step(val_loader, print_step=400)\n",
      "  File \"<ipython-input-479-075ce0d10d0c>\", line 157, in val_step\n",
      "    for i, databatch in enumerate(val_loader):\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 345, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 841, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 808, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 761, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 294, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'FileNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/inspect.py\", line 1465, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 182, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/tokenize.py\", line 394, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/tokenize.py\", line 363, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"/home/ntrong/.conda/envs/myenv/lib/python3.8/tokenize.py\", line 321, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-483-8d262aaa8ee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincre_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_part\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-479-075ce0d10d0c>\u001b[0m in \u001b[0;36mincre_update\u001b[0;34m(self, data_batch, stat_batch, val_batch)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# VALID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-479-075ce0d10d0c>\u001b[0m in \u001b[0;36mval\u001b[0;34m(self, val_batch)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_newusers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_newquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_newlectures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-479-075ce0d10d0c>\u001b[0m in \u001b[0;36mval_step\u001b[0;34m(self, val_loader, print_step, msg)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mval_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSocketClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FileNotFoundError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2044\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2046\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2047\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1435\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1436\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1336\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1193\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "params.load_state = False\n",
    "# save_part = None\n",
    "if params.load_state:\n",
    "    print(f'LOADING ALL DATASET ...')\n",
    "    mydata = LectureData(params) \n",
    "    trainer = Trainer(mydata, params)\n",
    "    \n",
    "    # filter data to speedup infer-time\n",
    "#     trainer.dataset.user_df = trainer.dataset.user_df.iloc[:1000]\n",
    "    trainer.dataset.lecture_question_scores = trainer.dataset.lecture_question_scores.iloc[-10000:]\n",
    "    trainer.dataset.user_question_tag_scores = trainer.dataset.user_question_tag_scores.iloc[-10000:]\n",
    "    trainer.dataset.user_question_part_scores = trainer.dataset.user_question_part_scores.iloc[-10000:]\n",
    "    \n",
    "    print('LOADING PRETRAINED MODEL ...')\n",
    "    trainer.load_model(os.path.join(params.extra_dir, 'model_latest.pth'))\n",
    "    \n",
    "else:\n",
    "    print(f'STARTING TRAINING FROM SCRATCH ...')\n",
    "    mydata = None\n",
    "    trainer = None\n",
    "    start = time.time()\n",
    "    for n, train_part in enumerate(chunks):\n",
    "        mini_chunks = gen_mini_chunks(train_part)\n",
    "        for train_part in mini_chunks:\n",
    "#             train_part = train_part.sample(frac=0.01)      \n",
    "            rest_part, valid_part = split_data(train_part, n_tail=6)\n",
    "            stat_part, train_part = split_data(rest_part, n_tail=18)\n",
    "        \n",
    "            print(f'Train: {train_part.shape} / Stats: {stat_part.shape} / Valid: {valid_part.shape}')\n",
    "            train_part = train_part.sort_values(by ='timestamp')\n",
    "\n",
    "            print(f'***Training chunk-{n}:')\n",
    "            if trainer is None:\n",
    "                mydata = LectureData(params, train_part, questions.copy(), lectures.copy())  \n",
    "                n_users, n_questions, n_lectures = mydata.n_users, mydata.n_questions, mydata.n_lectures\n",
    "                params.update(**{\n",
    "                    'num_users': n_users,\n",
    "                    'num_questions': n_questions,\n",
    "                    'num_lectures': n_lectures\n",
    "                })\n",
    "                print(f'[Train] All cases: {n_users}, {n_questions}, {n_lectures}')\n",
    "                trainer = Trainer(mydata, params)\n",
    "    #             trainer = GBMTrainer(mydata, params)\n",
    "\n",
    "            else:\n",
    "#                 train_part = convert_train_to_test(train_part, group=n)\n",
    "#                 s1 = time.time()\n",
    "#                 pred = trainer.infer(train_part)\n",
    "#                 trainer.finetune_batch()\n",
    "#                 print(f'Testing Time: {time.time() - s1}')\n",
    "#                 continue\n",
    "\n",
    "                trainer.incre_update(train_part, stat_part, valid_part)\n",
    "\n",
    "        if n >= params.n_chunks:\n",
    "            break\n",
    "                \n",
    "        print(f'Batch-Time elapsed: {time.time() - start}')\n",
    "\n",
    "    print(f'Training finished in {time.time() - start} seconds')\n",
    "    \n",
    "    #--------------\n",
    "    trainer.save_model('./save/model_latest.pth')\n",
    "    dump_all(trainer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2ce63614-a288-4be0-ae66-49df369340ca",
    "_uuid": "7bbfc78f-7707-4d61-9bf1-84dc7228a516"
   },
   "outputs": [],
   "source": [
    "import riiideducation\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = riiideducation.make_env()\n",
    "\n",
    "# You can only iterate through a result from `env.iter_test()` once\n",
    "# so be careful not to lose it once you start iterating.\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Submission\n",
    "################################\n",
    "\n",
    "print(f'Start testing ....')\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    # do prediction\n",
    "    pred = trainer.infer(test_df)\n",
    "\n",
    "    # fill all value first\n",
    "    test_df['answered_correctly'] = 0.5\n",
    "    \n",
    "    # Only fill question-type\n",
    "    test_df.loc[test_df['content_type_id'] == 0, 'answered_correctly'] = pred\n",
    "\n",
    "    # submit prediction    \n",
    "    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n",
    "    \n",
    "    trainer.finetune_batch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
